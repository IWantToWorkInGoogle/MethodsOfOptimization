\chapter{Теоретическая часть}
\label{ch:chap1}

    В этом разделе будут описаны все вычислительные схемы требующихся методов.

\section{Градиентный спуск с постоянным шагом (learning rate)}

    Принцип работы: 

    Суть метода градиентного спуска заключается в том, чтобы идти в направлении скорейшего спуска. Таким образом, за достаточное количество шагов можно дойти до локального минимума функции, если таковой существует.
    

    Вход: функция $f:\mathbb{R}^n \rightarrow \mathbb{R}$, стартовая точка $x = (x_1,x_2,...,x_n)$, точность $\varepsilon$, размер шага $\lambda$
    
    Выход: найденная точка локального минимума

    Алгоритм:
    \begin{enumerate}
        \item $x^{[k+1]} = x^{[k]} - \lambda\nabla f(x^{[k]})$.
        \item Повторять шаг 1, пока  $|x^{[k+1]} - x^{[k]}| > \varepsilon$.
    \end{enumerate}

    
    
\section{Метод дихотомии}

    Принцип работы:
    

    Вход: функция $f:\mathbb{R} \rightarrow \mathbb{R}$, стартовая точка $x$, точность $\varepsilon$, отрезок, на котором ищется минимум $[a ,b]$
    
    Выход: найденная точка локального минимума

    Алгоритм(для поиска минимума):
    \begin{enumerate}
        \item На каждом шаге процесса поиска делим отрезок $[a,b]$ пополам, $x = \frac{a + b}{2}$ - координата середины отрезка $[a,b]$.
        \item $x_1 := \frac{a + b}{2} + \delta, x_2 := \frac{a + b}{2} - \delta, \delta \in (0; \frac{a - b}{2})$
        \item Вычисляем значение функции $f$ в $x_1$ и $x_2$: 

        $F_1 = f(x_1), F_2 = f(x_2)$

        \item Сравниваем $F_1$ и $F_2$ и выбираем отрезок для дальнейшего рассмотрения:
        \begin{itemize}
            \item Если $F_1 < F_2$ , то отбрасываем отрезок $[x_2,b]$, тогда $b = x_2$, рассматриваем $[a, x_2]$.
            \item Иначе рассматриваем $[x_1, b]$.
        \end{itemize}

        \item Деление отрезка $[a,b]$ продолжается до тех пор, пока его длина не станет меньше заданной точности $\varepsilon$, т.е. $|b - a| \leq\varepsilon$
    \end{enumerate}


\section{Градиентный спуск на основе метода дихотомии}

    Принцип работы:

жюдююжд    Принцип работы данной модификации абсолютно схож с вычислительной схемой обычного градиентного спуска, кроме одного пункта. Вместо того, чтобы прыгать на опрделенный шаг, для начала мы найдем точку минимума функции на одномерном отрезке $[x^{[k]}, x^{[k]} + \lambda\nabla f(x^{[k]})]$. Этот прием поможет сходиться с ответом в некоторых случаях за меньшее количество итераций, но количество процессорного времени, затраченного алгоритмом, будет выше, чем для обычного градиентного спуска для достижения высокой точности (будет проиллюстрировано в практической части).

    Алгоритм:
    
    Вход: функция $f:\mathbb{R}^n \rightarrow \mathbb{R}$, стартовая точка $x$, точность $\varepsilon$, размер шага $\lambda$
    \begin{enumerate}
        \item $x^{'[k]} = x^{[k]} - \lambda\nabla f(x^{[k]})$.
        \item Найти с помощью метода дихотомии локальный минимум на отрезке $[x^{[k]}, x^{'[k]}]$. Обозначим этот минимум как $x^{[k + 1]}$
        \item Повторять шаги 1-2, пока  $|x^{[k+1]} - x^{[k]}| > \varepsilon$.
    \end{enumerate}


\section{Оптимизация градиентного спуска с помощью условия Вольфе}

    Принцип работы:
    У градиентного спуска с постоянным шагом есть недостаток: если алгоритм находится слишком близко к точке минимума, то с постоянным шагом алгоритм может "перескочить" через точку экстремума, что приведет к тому, что будет затрачено больше итераций, чем нужно. Поэтому стоит ввести механизм изменения размера шага. В данном случае мы будем использовать критерий Вольфе.

    Определение критерия Вольфе:

    $p$ - направление, в котором мы хотим изменить $x$, в случае градиентного спуска - антиградиент функции в точке $x$

    $f(x + \alpha p) \leq f(x) + c_1 \alpha \nabla f^T p$

    $|\nabla f(x + \alpha p)^T p| \geq |c_2 \nabla f^T p|$

    $0 < c_1 < c_2 < 1$, $c_1$ близко к $0$, $c_2$ близко к $1$

    
    
    Вход: функция $f:\mathbb{R}^n \rightarrow \mathbb{R}$, стартовая точка $x$, точность $\varepsilon$, начальный шаг $\alpha_0$

    Выход: найденная точка локального минимума

    Алгоритм:
    \begin{enumerate}
        \item $\alpha = \alpha_0$
        \item Уменьшаем $\alpha$ до того момента, пока критерий Вольфе не начнет исполняться.
        \item $x^{[k + 1]} = x^{[k]} - \alpha\nabla f(x^{[k]})$.
        \item Повторять шаги, пока  $|x^{[k+1]} - x^{[k]}| > \varepsilon$.
    \end{enumerate}
    

\endinput